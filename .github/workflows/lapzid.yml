## 1ï¸âƒ£ GitHub Actions Workflow (`.github/workflows/lapizid.yml`) â€” **EKSÄ°KSÄ°Z**

```yaml
name: Lapizid M3U OluÅŸtur

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Repoyu Ä°ndir
        uses: actions/checkout@v4

      - name: Python Kur
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: KÃ¼tÃ¼phaneleri YÃ¼kle
        run: pip install cloudscraper requests beautifulsoup4

      - name: M3U DosyasÄ±nÄ± OluÅŸtur
        run: python lapizid.py

      - name: DeÄŸiÅŸiklikleri Kaydet
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add dizipal_filmler.m3u
          git commit -m "âœ… dizipal_filmler.m3u gÃ¼ncellendi" || echo "DeÄŸiÅŸiklik yok"
          git push
```

---

## 2ï¸âƒ£ Python Script (`lapizid.py`) â€” **TAM, EKSÄ°KSÄ°Z, DEBUG KAPALI**

```python
#!/usr/bin/env python3
"""
DÄ°ZÄ°PAL TAM FÄ°LM SCRAPER
- TÃœM KATEGORÄ°LER
- TÃœM YILLAR (2025 â†’ 1960)
- SAYFALAMA DESTEÄÄ°
- LOGO (og:image + fallback)
- M3U OTOMATÄ°K ÃœRETÄ°M
- GITHUB ACTIONS UYUMLU
"""

import cloudscraper
import requests
import re
import time
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup

class DizipalScraper:
    def __init__(self):
        self.base_url = self.get_current_domain()
        print(f"ğŸ”— GÃ¼ncel Domain: {self.base_url}")

        self.scraper = cloudscraper.create_scraper()
        self.scraper.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8",
            "Referer": self.base_url
        })

        # 2025 â†’ 1960
        self.years = list(range(2025, 1959, -1))

        # TÃœM FÄ°LM TÃœRLERÄ° (EKSÄ°KSÄ°Z)
        self.film_turleri = {
            "aile": "aile",
            "aksiyon": "aksiyon",
            "animasyon": "animasyon",
            "anime": "anime",
            "belgesel": "belgesel",
            "bilimkurgu": "bilimkurgu",
            "biyografi": "biyografi",
            "dram": "dram",
            "editorun-sectikleri": "editorun-sectikleri",
            "erotik": "erotik",
            "fantastik": "fantastik",
            "gerilim": "gerilim",
            "gizem": "gizem",
            "komedi": "komedi",
            "korku": "korku",
            "macera": "macera",
            "mubi": "mubi",
            "muzik": "muzik",
            "romantik": "romantik",
            "savas": "savas",
            "spor": "spor",
            "suc": "suc",
            "tarih": "tarih",
            "western": "western",
            "yerli": "yerli"
        }

    # --------------------------------------------------
    # GÃœNCEL DOMAIN
    # --------------------------------------------------
    def get_current_domain(self):
        try:
            url = "https://raw.githubusercontent.com/mehmetey03/doma/refs/heads/main/lapiziddomain.txt"
            r = requests.get(url, timeout=10)
            for line in r.text.splitlines():
                if line.startswith("guncel_domain="):
                    return line.split("=", 1)[1].strip().rstrip("/")
        except Exception:
            pass
        return "https://dizipal1222.com"

    # --------------------------------------------------
    # KATEGORÄ° TARAMA
    # --------------------------------------------------
    def crawl_category(self, tur_name, tur_slug):
        print(f"\nğŸ¬ KATEGORÄ°: {tur_name.upper()}")
        results = []

        for year in self.years:
            print(f"   ğŸ“… YÄ±l: {year}")
            page = 1

            while True:
                encoded = quote(f"/tur/{tur_slug}?", safe="")
                url = f"{self.base_url}/tur/{tur_slug}?genre={encoded}&yil={year}&kelime=&sayfa={page}"
                print(f"      ğŸ“„ Sayfa {page}")

                try:
                    r = self.scraper.get(url, timeout=30)
                    if r.status_code != 200:
                        break

                    soup = BeautifulSoup(r.text, "html.parser")
                    container = soup.find("article", class_="type2")
                    if not container:
                        break

                    items = container.find_all("li")
                    if not items:
                        break

                    for li in items:
                        a = li.find("a")
                        if not a:
                            continue

                        href = a.get("href", "")
                        if "/film/" not in href:
                            continue

                        film_url = urljoin(self.base_url, href)
                        film = self.parse_film(film_url, tur_name, year)
                        if film:
                            results.append(film)

                    page += 1
                    time.sleep(0.4)

                except Exception:
                    break

        print(f"   âœ… {tur_name.upper()} toplam: {len(results)} film")
        return results

    # --------------------------------------------------
    # FÄ°LM DETAY
    # --------------------------------------------------
    def parse_film(self, film_url, tur_name, year):
        try:
            r = self.scraper.get(film_url, timeout=30)
            if r.status_code != 200:
                return None

            soup = BeautifulSoup(r.text, "html.parser")

            # BaÅŸlÄ±k
            title = "Bilinmeyen Film"
            if soup.title:
                title = soup.title.text
                title = title.replace(" Ä°zle | dizipal", "").replace(" | dizipal", "").strip()

            # Logo
            logo = ""
            og = soup.find("meta", property="og:image")
            if og:
                logo = og.get("content", "")

            if not logo:
                cover = soup.find("div", class_="cover")
                if cover and "style" in cover.attrs:
                    m = re.search(r"url\((https?://[^)]+)\)", cover["style"])
                    if m:
                        logo = m.group(1)

            # tvg-id
            clean = re.sub(r"[^a-z0-9]+", "_", title.lower()).strip("_")
            tvg_id = f"{clean}_{year}"

            return {
                "title": f"{title} ({year})",
                "url": film_url,
                "logo": logo,
                "tvg_id": tvg_id,
                "group": f"Film - {tur_name.upper()}"
            }
        except Exception:
            return None

    # --------------------------------------------------
    # TÃœM KATEGORÄ°LER
    # --------------------------------------------------
    def run_films_only(self):
        all_films = []
        for tur_name, tur_slug in self.film_turleri.items():
            films = self.crawl_category(tur_name, tur_slug)
            all_films.extend(films)
            time.sleep(1)

        self.write_m3u(all_films)

    # --------------------------------------------------
    # M3U YAZMA
    # --------------------------------------------------
    def write_m3u(self, films):
        lines = ['#EXTM3U x-tvg-url="https://github.com/botallen/epg/releases/download/latest/epg.xml"']

        for film in films:
            lines.append(
                f'#EXTINF:-1 tvg-id="{film["tvg_id"]}" '
                f'tvg-name="{film["title"]}" '
                f'tvg-logo="{film["logo"]}" '
                f'group-title="{film["group"]}",{film["title"]}'
            )
            lines.append(film["url"])

        with open("dizipal_filmler.m3u", "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print("=" * 60)
        print(f"âœ… M3U OLUÅTURULDU | Toplam Film: {len(films)}")
        print("ğŸ“ dosya: dizipal_filmler.m3u")
        print("=" * 60)


# --------------------------------------------------
# Ã‡ALIÅTIRMA
# --------------------------------------------------
if __name__ == "__main__":
    scraper = DizipalScraper()
    scraper.run_films_only()
```

---

## âœ… GARANTÄ°

* âŒ DEBUG yok
* âŒ Kod kesilmedi
* âŒ Fonksiyon atlanmadÄ±
* âœ” TÃ¼m kategoriler
* âœ” TÃ¼m yÄ±llar
* âœ” Sayfalama
* âœ” Logo
* âœ” M3U Ã¼retim

Bu **ÅŸu ana kadar verdiÄŸim en tam sÃ¼rÃ¼m**.
Ä°stersen sonraki adÄ±mda **dizi ekleyelim**, **thread pool**, **duplicate temizleme** veya **kategoriye gÃ¶re ayrÄ± M3U** yapalÄ±m.
