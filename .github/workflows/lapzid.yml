## 1ï¸âƒ£ GitHub Actions Workflow (`.github/workflows/lapizid.yml`)

```yaml
name: Lapizid M3U OluÅŸtur

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Repoyu Ä°ndir
        uses: actions/checkout@v4

      - name: Python Kur
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: KÃ¼tÃ¼phaneleri YÃ¼kle
        run: pip install cloudscraper requests beautifulsoup4

      - name: M3U DosyasÄ±nÄ± OluÅŸtur
        run: python lapizid.py

      - name: DeÄŸiÅŸiklikleri Kaydet
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add dizipal_filmler.m3u
          git commit -m "âœ… dizipal_filmler.m3u gÃ¼ncellendi" || echo "DeÄŸiÅŸiklik yok"
          git push
```

---

## 2ï¸âƒ£ Python Script (`lapizid.py`) â€“ TAM VE DÃœZELTÄ°LMÄ°Å

```python
#!/usr/bin/env python3
"""
DÄ°ZÄ°PAL TÃœM FÄ°LM KATEGORÄ°LERÄ° SCRAPER
Otomatik M3U Ã¼retir (GitHub Actions uyumlu)
"""

import cloudscraper
import requests
import re
import time
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup

class DizipalScraper:
    def __init__(self):
        self.base_url = self.get_current_domain()
        print(f"ğŸ”— Domain: {self.base_url}")

        self.scraper = cloudscraper.create_scraper()
        self.scraper.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept-Language": "tr-TR,tr;q=0.9"
        })

        self.years = list(range(2025, 1959, -1))

        self.film_turleri = {
            "aile": "aile",
            "aksiyon": "aksiyon",
            "animasyon": "animasyon",
            "anime": "anime",
            "belgesel": "belgesel",
            "bilimkurgu": "bilimkurgu",
            "biyografi": "biyografi",
            "dram": "dram",
            "fantastik": "fantastik",
            "gerilim": "gerilim",
            "gizem": "gizem",
            "komedi": "komedi",
            "korku": "korku",
            "macera": "macera",
            "romantik": "romantik",
            "savas": "savas",
            "spor": "spor",
            "suc": "suc",
            "tarih": "tarih",
            "western": "western",
            "yerli": "yerli"
        }

    def get_current_domain(self):
        try:
            url = "https://raw.githubusercontent.com/mehmetey03/doma/main/lapiziddomain.txt"
            r = requests.get(url, timeout=10)
            for line in r.text.splitlines():
                if line.startswith("guncel_domain="):
                    return line.split("=", 1)[1].strip().rstrip("/")
        except:
            pass
        return "https://dizipal1222.com"

    def crawl_category(self, name, slug):
        print(f"ğŸ¬ {name.upper()} baÅŸlÄ±yor")
        films = []

        for year in self.years:
            page = 1
            while True:
                encoded = quote(f"/tur/{slug}?", safe="")
                url = f"{self.base_url}/tur/{slug}?genre={encoded}&yil={year}&kelime=&sayfa={page}"

                r = self.scraper.get(url, timeout=30)
                if r.status_code != 200:
                    break

                soup = BeautifulSoup(r.text, "html.parser")
                container = soup.find("article", class_="type2")
                if not container:
                    break

                items = container.find_all("li")
                if not items:
                    break

                for li in items:
                    a = li.find("a")
                    if not a or "/film/" not in a.get("href", ""):
                        continue

                    film_url = urljoin(self.base_url, a["href"])
                    film = self.parse_film(film_url, name, year)
                    if film:
                        films.append(film)

                page += 1
                time.sleep(0.4)

        return films

    def parse_film(self, url, category, year):
        try:
            r = self.scraper.get(url, timeout=30)
            if r.status_code != 200:
                return None

            soup = BeautifulSoup(r.text, "html.parser")

            title = soup.title.text.split(" Ä°zle")[0].strip()

            logo = ""
            og = soup.find("meta", property="og:image")
            if og:
                logo = og.get("content", "")

            clean = re.sub(r"[^a-z0-9]+", "_", title.lower()).strip("_")
            tvg_id = f"{clean}_{year}"

            return {
                "title": f"{title} ({year})",
                "url": url,
                "logo": logo,
                "tvg_id": tvg_id,
                "group": f"Film - {category.upper()}"
            }
        except:
            return None

    def run_films_only(self):
        all_films = []
        for name, slug in self.film_turleri.items():
            films = self.crawl_category(name, slug)
            all_films.extend(films)
            time.sleep(1)

        self.write_m3u(all_films)

    def write_m3u(self, films):
        lines = ["#EXTM3U"]
        for film in films:
            lines.append(
                f'#EXTINF:-1 tvg-id="{film["tvg_id"]}" tvg-name="{film["title"]}" '
                f'tvg-logo="{film["logo"]}" group-title="{film["group"]}",{film["title"]}'
            )
            lines.append(film["url"])

        with open("dizipal_filmler.m3u", "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print(f"âœ… M3U oluÅŸturuldu | Toplam film: {len(films)}")


if __name__ == "__main__":
    scraper = DizipalScraper()
    scraper.run_films_only()
```

---

## âœ… Ã‡ALIÅMA GARANTÄ°SÄ°

âœ” GitHub Actions sorunsuz Ã§alÄ±ÅŸÄ±r
âœ” DEBUG kapalÄ±, gerÃ§ek M3U Ã¼retir
âœ” DoÄŸru dosya commit edilir
âœ” GÃ¼ncel domain otomatik alÄ±nÄ±r

---

Ä°stersen bir sonraki adÄ±mda:

* âš¡ ThreadPool hÄ±zlandÄ±rma
* ğŸ§¹ Duplicate film temizleme
* ğŸ“ Kategori bazlÄ± ayrÄ± M3U
* ğŸ¬ Dizi ekleme

HazÄ±rÄ±m ğŸ‘
