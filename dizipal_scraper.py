def crawl_film_category_correct(self, tur_name, tur_slug):
    """DOÄžRU ÅžEKÄ°LDE: Film kategorisini tÃ¼m yÄ±llar iÃ§in Ã§ek"""
    print(f"\nðŸŽ¬ FÄ°LM KATEGORÄ°SÄ°: {tur_name.upper()} (Slug: {tur_slug})")
    
    all_films = []
    
    # Her yÄ±l iÃ§in ayrÄ± ayrÄ± tarama
    for year in self.years:
        print(f"   ðŸ“… YÄ±l: {year}")
        
        # DÃœZELTME: BASÄ°T URL YAPISI KULLAN
        # HTML'de gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi: /tur/aksiyon?
        base_url = f"{self.base_url}/tur/{tur_slug}?yil={year}"
        
        page = 1
        year_films_count = 0
        
        while True:
            # Sayfa numarasÄ±nÄ± ekle
            if page == 1:
                url = base_url
            else:
                url = f"{base_url}&sayfa={page}"
            
            print(f"      ðŸ“„ Sayfa {page}: {url[:80]}...")
            
            try:
                r = self.make_request(url, timeout=30)
                
                if not r or r.status_code != 200:
                    status_code = getattr(r, 'status_code', 'Bilinmiyor')
                    print(f"      âŒ HTTP HatasÄ± {status_code}")
                    break
                
                soup = BeautifulSoup(r.content, 'html.parser')
                
                # DEÄžÄ°ÅžÄ°KLÄ°K: FarklÄ± HTML yapÄ±sÄ± iÃ§in seÃ§icileri gÃ¼ncelle
                # Ã–nce sayfanÄ±n yapÄ±sÄ±nÄ± kontrol et
                print(f"      ðŸ” HTML analizi...")
                
                # 1. FarklÄ± seÃ§iciler deneyelim
                film_links = []
                
                # SeÃ§enek 1: div iÃ§indeki film linkleri
                movie_divs = soup.select('div.movie-item, div.film-item, div.item')
                for div in movie_divs:
                    links = div.select('a[href*="/film/"]')
                    for link in links:
                        href = link.get('href', '')
                        if href and '/film/' in href:
                            full_url = urljoin(self.base_url, href)
                            if full_url not in film_links:
                                film_links.append(full_url)
                
                # SeÃ§enek 2: Direkt tÃ¼m film linkleri
                if not film_links:
                    all_links = soup.find_all('a', href=lambda x: x and '/film/' in x)
                    for link in all_links:
                        href = link.get('href', '')
                        full_url = urljoin(self.base_url, href)
                        if full_url not in film_links:
                            film_links.append(full_url)
                
                # SeÃ§enek 3: data-href veya data-url attribute'larÄ±
                if not film_links:
                    data_links = soup.find_all(attrs={"data-href": lambda x: x and '/film/' in str(x)})
                    for elem in data_links:
                        href = elem.get('data-href')
                        if href:
                            full_url = urljoin(self.base_url, href)
                            if full_url not in film_links:
                                film_links.append(full_url)
                
                print(f"      âœ… {len(film_links)} film linki bulundu")
                
                # HTML'de hata ayÄ±klama iÃ§in ilk 1000 karakteri gÃ¶ster
                if not film_links and page == 1:
                    print(f"      ðŸ› DEBUG: Sayfa iÃ§eriÄŸinin ilk 1000 karakteri:")
                    print(str(soup)[:1000])
                
                if not film_links:
                    if page == 1:
                        print(f"      âš ï¸  {year} yÄ±lÄ± iÃ§in film bulunamadÄ±")
                    break
                
                # 3. Her film iÃ§in bilgileri Ã§ek (ilk 3 filmle sÄ±nÄ±rla test iÃ§in)
                for film_url in film_links[:3]:  # Test iÃ§in sadece ilk 3 film
                    try:
                        r2 = self.make_request(film_url, timeout=30)
                        
                        if not r2 or r2.status_code != 200:
                            continue
                        
                        soup2 = BeautifulSoup(r2.content, 'html.parser')
                        
                        # Film baÅŸlÄ±ÄŸÄ±nÄ± al
                        title_tag = soup2.find('title')
                        if title_tag:
                            title_text = title_tag.text
                            if ' Ä°zle |' in title_text:
                                film_title = title_text.split(' Ä°zle |')[0].strip()
                            elif ' | dizipal' in title_text:
                                film_title = title_text.split(' |')[0].strip()
                            else:
                                film_title = title_text.strip()
                        else:
                            film_title = "Bilinmeyen Film"
                        
                        # Logoyu al
                        logo = ""
                        meta_image = soup2.find('meta', property='og:image')
                        if meta_image:
                            logo = meta_image.get('content', '')
                        
                        # Alternatif logo kaynaÄŸÄ±
                        if not logo:
                            poster_img = soup2.find('div', class_='cover')
                            if poster_img and 'style' in poster_img.attrs:
                                style = poster_img['style']
                                logo_match = re.search(r'url\((https://[^)]+)\)', style)
                                if logo_match:
                                    logo = logo_match.group(1)
                        
                        # YÄ±lÄ± URL'den Ã§Ä±kar
                        year_match = re.search(r'(\d{4})', film_url)
                        film_year = year_match.group(1) if year_match else str(year)
                        
                        # tvg-id oluÅŸtur
                        clean_title = re.sub(r'[^\w\s-]', '', film_title.lower())
                        clean_title = clean_title.replace(' ', '_').replace('__', '_')
                        tvg_id = f"{clean_title}_{film_year}"
                        
                        all_films.append({
                            'url': film_url,
                            'title': f"{film_title} ({film_year})",
                            'tvg_id': tvg_id,
                            'logo': logo,
                            'group_title': f"Film - {tur_name.upper()}",
                            'type': 'film'
                        })
                        
                        year_films_count += 1
                        print(f"         âœ… Ä°ÅŸlendi: {film_title}")
                        
                    except Exception as e:
                        print(f"         âŒ Film bilgisi alÄ±namadÄ±: {str(e)[:50]}")
                        continue
                
                # 4. Sonraki sayfa kontrolÃ¼
                next_page = soup.select_one('a[rel="next"], .pagination .next, a:contains("Sonraki")')
                if not next_page:
                    break
                
                page += 1
                time.sleep(1)  # Sunucu yÃ¼kÃ¼nÃ¼ azalt
                
            except Exception as e:
                print(f"      âŒ {year} - Sayfa {page} hatasÄ±: {str(e)[:50]}")
                break
        
        print(f"      ðŸ“Š {year} yÄ±lÄ±: {year_films_count} film")
        
        # Her yÄ±l arasÄ±nda biraz bekle
        if year_films_count > 0:
            time.sleep(2)
    
    print(f"   ðŸ“Š Kategori toplam: {len(all_films)} film")
    return all_films
